# env
env_id: "LunarLander-v2"
sparse_reward: true

# seed
seed: 4076

# replay buffer
warmup_with_random_trajectories: true
num_warmup_episodes: 10    # number of warm-up episodes at the beginning
replay_buffer_size: 300   # max size of the replay buffer (in episodes)

# policy
deterministic_policy: false
hidden_size: 1024

# command
command_dim: 2
return_scale: 0.01  # scaling factor for desired horizon input (reward)
horizon_scale: 0.01  # scaling factor for desired horizon input (steps)

# train
loss_type: "log_prob" # "mse"
batch_size: 512   # batch size per update
learning_rate: 1e-3  # lr for ADAM optimizer
num_iterations: 1000
num_updates_per_iter: 100   # number of gradient-based updates per iteration
num_episodes_per_iter: 10    # number of episodes generated per iteration
last_few: 25    # number of episodes from the end of the replay buffer 
                              # used for sampling exploratory commands