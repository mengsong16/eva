# env
env_id: "LunarLander-v2"
#env_id: "Swimmer-v2"
sparse_reward: true

# seed
seed: 4076 #1

# cuda
gpu_id: 0

# log
log_to_wandb: true

# replay buffer
warmup_with_random_trajectories: true
num_warmup_episodes: 10    # number of warm-up episodes at the beginning
replay_buffer_size: 300   # max size of the replay buffer (in episodes)

# policy
deterministic_policy: false #false
hidden_size: 1024

# teacher
algorithm_name: "udrl"
target_dim: 2
target_type: ["return", "horizon"]
num_top_episodes: 25    # number of top episodes selected from the replay buffer 
min_target_horizon: 200 # lower bound of target horizon
return_scale: 0.01  # scaling factor for desired return input
horizon_scale: 0.01  # scaling factor for desired horizon input

# train
loss_type: "log_prob" #"log_prob" # "mse"
batch_size: 512   # batch size per update
learning_rate: 1e-3  # learning rate for ADAM optimizer
num_train_iterations: 500 #500
num_updates_per_iter: 100   # number of gradient-based updates per iteration
num_train_episodes_per_iter: 5 # number of episodes used for training per iteration
num_new_episodes_per_iter: 10    # number of new episodes generated per iteration

# save and load
save_every_iterations: 100

# evaluation
eval_checkpoint_folder: "udrl-LunarLander-v2-sparse-20220330-211714"
eval_checkpoint_index: 3
num_eval_episodes: 50