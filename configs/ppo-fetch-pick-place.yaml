# basic
algorithm: "PPO"
env_id: "FetchPickAndPlace-v1" #"bitflip-v0" #"umaze-v0"


# seed
seed: 1

# cuda
gpu_id: 0

# vec env
n_envs: 4 #6 

# encoder
output_dim: 512
hidden_dim: 512
hidden_layer: 2

# goal
goal_format: "absolute" #"relative" #"absolute"

# training
total_timesteps: 1000000 # how many steps to collect

# Evaluation
num_test_episodes: 100
eval_exp_name: "s1-20220716-130257"
  
# ppo
gamma: 0.99  # discount factor
learning_rate: 0.0003
n_steps: 2048  # n_envs * n_steps = rollout buffer size
batch_size: 128 #64  # batch size
n_epochs: 8 #10  # how many times the steps in rollout buffer are used to update the networks
gae_lambda: 0.95 
clip_range: 0.2
ent_coef: 0.0
